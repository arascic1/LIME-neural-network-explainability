This is an accompanying code repository for my undergraduate thesis dedicated to exploring the machine learning interpretability method LIME (Local Interpretable Model-agnostic Explanations) and comparing it to methods Anchors and SHAP (SHapley Additive exPlanations). This repository contains notebooks that were used to set up and create explanations on the ImageNet-trained ResNet50 deep neural network by all three interpretability methods. They can be run in the Google Collab environment and modified according to need. The images that were used for generating explanations were uploaded to the Google Collab session itself. It is recommended to run the methods utilizing GPUs since otherwise only limited explanations can be created in reasonable time. 
